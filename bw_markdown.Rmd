---
title: "EdX Capstone Project, Ben Waetford"
output:
  pdf_document: default
  html_document: default
date: "May 2022"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1.0 Introduction

The movielens dataset used in this project contains 10 million records. Each record contains the rating that a person has given to that movie between 1995 and 2009. The objective of this project is to construct a machine learning model that, using a subset of data for training, can predict the rating that individual users have given to the movies not contained in the training set.

The techniques used in this project are largely drawn from the EdX Data Science Professional Certificate. Other techniques used have been found by the author from experimentation, from reviewing help files, and from sources such as YouTube and online forums including towardsdatascience.com.

According to the courses grading rubric, the machine learning model must result in an RMSE \< 0.86490. The code created and detailed in this report achieved an RMSE of **0.8648170 -- a successful result, since it is less than the limit required by the Edx team**. The code resulting in this RSME can be found in this report. Also included in this report is a summary analysis of the data, as well as concluding remarks.

# 2.0 Methods and Analysis

## 2.1 Methods

The HarvardX Data Science Professional Certificate includes eight instructor led modules, plus a final captsone project. Each module focuses on different skills. Although this project requires skills learned in each model, the most important to this author were:

-   Visualization methods and techniques for visualizing data using the ggplot2 package.

-   Productivity tools--specifically Git.

-   Machine learning--specifically the formula **Y\_{u, i} = \\mu + \\epsilon\_{u, i}** that calculates RMSE on large datasets, as well as the technique for regularization, which is provided in section 34.9.2 of the course materials (Irizarry, 2020).

## 2.2 Data Preparation

### 2.2.1 EdX code

The course provided code to download the movielens data, to format it, and to split it into two data sets. One set being the holdout set which is only used after the model is built, for the purpose of validating the model. The other data is the training data which is used to train the model.

```{r Run code provided by the course creators}

##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()

download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")


# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)



```

### 2.2.2 Subset the edx data set further, to create a training set and validation set for model build and testing

According to the instructions provided by the course instructor, the validation set created by the EdX code may **not** be used when training the model. Since the EdX validation set is off-limits during the model training, the edx training set was split into two sets: edxTrain, and edxValidate. Subsequent analysis was based on edxTrain, and the model was constructed with wedxTrain and tested with edxValidate.

```{r My code to generate subsets from edx}

#load libraries

library(ggplot2)
library(corrplot)
library(dplyr)
library(lubridate)
library(moments)


#load data
# change timestamp into date data type, and create new variables from the date

myedx <- edx %>%
  mutate(reviewdt = as.POSIXct(edx$timestamp,origin = "1970-01-01"),
         review_y = round_date(reviewdt, unit = "year"), #the year the review was made
         INTreview_y = as.integer(year(reviewdt)), #review year as an integer to calculate span later
         review_ym = round_date(reviewdt, unit = "month"), #the yearmonth the review was made
         review_ymd = round_date(reviewdt, unit = "day"), #the yearmonthday the review was made
         release_y = as.integer(sub("\\).*", "", sub(".*\\(", "", edx$title))), #the year the film was released
         span_y = (INTreview_y - release_y), #the approximate age (in years) of the movie when it was rated
         howManyGenres = str_count(edx$genres, "\\|")+1) #the number of genres associated with the film


#create a test set and validation set from edx

set.seed(1976, sample.kind="Rounding") 
test_index <- createDataPartition(y = myedx$rating, times = 1, p = 0.7, list = FALSE)
edxValidate <- myedx[-test_index,] #my validation set
edxTrain <- myedx[test_index,] #my training set


```

## 2.3 Analysis

In this section, the Edx training dataset is use to produce summary statistics as a proxy summary statisitics that represent the enitre 10M dataset..

Since the movielens data set used in this project contains ratings users have given movies, it is important to find out what distinct ratings exist in the data set provided by EdX. Out of interest, the author also wanted to know how many times each rating was given to a movie. The dyplr package provides a method of grouping and counting rows. The data contains 10 ratings, with 4.0 being the most common rating, thus:

```{r Distinct ratings}
library("dplyr")

edxTrain %>%
  select(rating) %>%
  group_by(rating) %>%
  summarize(number_of_times_used = n())
```

ggplot2 can be used in conjunction with dyplr to view this information graphically:

```{r Distinct ratings -- vizualization}

edxTrain %>% 
  select(rating) %>%
  group_by(rating) %>%
  summarize(n = n()) %>%
  ggplot(aes(rating, n)) +
  geom_col() 

```

However, the fact that half-point ratings are available causes the trend to be more difficult to measure that it would otherwise be if half-point ratings were not possible. To make the trend easier to understand the half-point ratings were rounded, and the resulting data was plotted.

```{r Ratings rounded to the nearest while number}

edxTrain %>% 
  select(rating) %>%
  mutate(rounded_rating = round(rating, digits = 0)) %>%
  group_by(rounded_rating) %>%
  summarize(n = n()) %>%
  ggplot(aes(rounded_rating, n)) +
  geom_point() +
  geom_smooth()

```

All ratings in the data set were made between 1995 and 2009m with the significant majority being made since the year 2000. In fact, the highest number of ratings were in 2000, with 1997, 2001 and 2005 coming in a close three-way-tie for second place.

```{r Ratings by year}

edxTrain %>%
  select(review_y) %>%
  group_by(review_y) %>%
  summarise(n = n()) %>%
  ggplot(aes(review_y, n)) +
  geom_col()
```

Before machinel learning model is used to predict movie ratings, correlations were investigated is a way of identifying if any engineered features should be included in the model:

```{r Correlation plot}

cordata <- edxTrain %>%
  select(-c(title, genres, timestamp, INTreview_y, reviewdt, review_y, review_ym, review_ymd)) #remove non numeric for correlation
C <- cor(cordata)
corrplot(C, method = 'number', type = 'lower') #nothing special noted in correlation plot

```

Checking for data elements that correlate with rating: there are no strong correlations. the strongest seem to be with release year and span (which is the number of years between the release year and the rating year). The correlations are weak, and therefore, the release year and the time between release and rating do not need to be included in the machine learning model. Note that non-numeric data elements were removed from the data set since only numeric data can be used with the cor() function.

```{r Specific correlations}

cor(edxTrain$rating, edxTrain$release_y)
cor(edxTrain$rating, edxTrain$span_y)

```

## 2.4 Training the Machine Learning Model

Code in this section is adapted from Irizarry (2020).

The film's release year, the time duration between the release of the movie and the review, and the number of genres the films are categorized in were computed in section 2.2 Data Preparation. According to section 2.3 Analysis, the computed features are not highly correlated with the film's ratings, thus; the engineered features are not included in the code generated to train the machine learning model (however, during training, the features were included for testing purposes, see the rmd file that accompanies this report to see the entire code base generate d for this assignment).

### 2.4.1 Building and training the model data generated in section 2.2.2; without regularization:

```{r}
#RMSE
mu <- mean(edxTrain$rating)
naive_rmse <- RMSE(edxTrain$rating, mu)

#results table
rmse_results <- data_frame(method = "Just the Average", RMSE = naive_rmse)

#effects
#movie effect
movie_avg <- edxTrain %>%
  group_by(movieId) %>%
  summarize(bmovie = mean(rating - mu))

predicted_ratings <- mu + edxValidate %>%
  left_join(movie_avg, by = "movieId") %>%
  .$bmovie

model_1_rmse <- RMSE(predicted_ratings, edxValidate$rating, na.rm = TRUE)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie effect Model",
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()


#user effect
user_avg <- edxTrain %>%
  group_by(userId) %>%
  summarize(buser = mean(rating - mu))

predicted_ratings <- edxValidate %>% 
  left_join(movie_avg, by='movieId') %>%
  left_join(user_avg, by='userId') %>%
  mutate(pred = mu + bmovie + buser) %>%
  .$pred

model_2_rmse <- RMSE(predicted_ratings, edxValidate$rating, na.rm = TRUE)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="+ User Effects Model",  
                                     RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()
```

2.4.2 Enhancing the model with regularization:

```{r}
lambdas <- seq(0, 10, 0.25)

mu <- mean(edxTrain$rating)
summ <- edxTrain %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())

rmses <- sapply(lambdas, function(l){
  predicted_ratings <- edxValidate %>% 
    left_join(summ, by='movieId') %>% 
    mutate(bmovie = s/(n_i+l)) %>%
    mutate(pred = mu + bmovie) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edxValidate$rating, na.rm = TRUE))
})

qplot(lambdas, rmses)  
lambdas[which.min(rmses)]

#Compute lambda for all effects (b_m, b_u, b_g)
rmses <- sapply(lambdas, function(l){
  b_m <- edxTrain %>%
    group_by(movieId) %>%
    summarise(b_m = sum(rating - mu)/(n()+l))
  b_u <- edxTrain %>%
    left_join(b_m, by="movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_m - mu)/(n()+l))

  predicted_ratings <- edxValidate %>%
    left_join(b_m, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    mutate(pred = mu + b_m + b_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edxValidate$rating, na.rm = TRUE))
})

lambda <- lambdas[which.min(rmses)]
qplot(lambdas, rmses)
model3 <-  min(rmses)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="+ Lambda Model",  
                                     RMSE = model3 ))
rmse_results %>% knitr::kable()
```

## 2.5 The Final Model

Now the Edx can be processed and the final RMSE an be reported.

```{r}
lambdas <- seq(0, 10, 0.25)

mu <- mean(edx$rating)
summ <- edx %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())

rmses <- sapply(lambdas, function(l){
  predicted_ratings <- validation %>% 
    left_join(summ, by='movieId') %>% 
    mutate(bmovie = s/(n_i+l)) %>%
    mutate(pred = mu + bmovie) %>%
    pull(pred)
  return(RMSE(predicted_ratings, validation$rating, na.rm = TRUE))
})

qplot(lambdas, rmses)  
lambdas[which.min(rmses)]

rmses <- sapply(lambdas, function(l){
  b_m <- edx %>%
    group_by(movieId) %>%
    summarise(b_m = sum(rating - mu)/(n()+l))
  b_u <- edx %>%
    left_join(b_m, by="movieId") %>%
    group_by(userId) %>%
    summarise(b_u = sum(rating - b_m - mu)/(n()+l))

  predicted_ratings <- validation %>%
    left_join(b_m, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    # left_join(b_g, by="genres") %>%
    #left_join(b_y, by = "review_y")
    mutate(pred = mu + b_m + b_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, validation$rating, na.rm = TRUE))
})

lambda <- lambdas[which.min(rmses)]
qplot(lambdas, rmses)
model3 <-  min(rmses)

rmse_results <- bind_rows(rmse_results,
                          data_frame(method="+ FINAL Lambda Model",  
                                     RMSE = model3 ))
rmse_results %>% knitr::kable()
```

## 

# 3.0 Conclusions

With large datasets, such as the 10M movie data used in this assignment it is possible to predict the ratings that a movie will be given by users. The accuracy of the predictions is improved when regularized to take movie effect and user effect into account.

It was possible to engineer additional features from the data provided. However, upon analysis, it was found that those additional features did not improve the accuracy of the predicted ratings--in fact, in many cases, accuracy suffered from including engineered features.

When analyzing data it is important to measure the effect of data elements that are included in machine learning models, and an analyst must be sure to measure the effect of new data elements introduced into machine learning models. More data does not necessarily result in better predictions.

**Reference.**

Irizarry, Rafael (2020) . *Introduction to Data Science: Data Analysis and Prediction Algorithms with R.* CHAPMAN & HALL/CRC DATA SCIENCE SERIES. Boca Raton: CRC Press, Taylor & Francis Group.
